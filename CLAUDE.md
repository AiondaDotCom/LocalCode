# LocalCode - Perl-based AI Coding Agent

![LocalCode in Action](example.png)

## Project Overview

LocalCode is an ultra-minimal, Perl-based AI coding agent designed for Ollama with advanced browser tools and gpt-oss model support. Built with Test-Driven Development for maximum reliability and autonomous testing. **Status: Production Ready** with 310+ tests passing.

## Architecture

**Core Philosophy:** Maximum functionality with minimal code (~1000 lines total)

### Modules Implemented ‚úÖ

1. **LocalCode::Config** (18 tests ‚úÖ) - YAML configuration with ~/.localcode persistence
2. **LocalCode::Permissions** (26 tests ‚úÖ) - SAFE/DANGEROUS/BLOCKED tool classification  
3. **LocalCode::Session** (20 tests ‚úÖ) - JSON-based session persistence with unified history
4. **LocalCode::Client** (15 tests ‚úÖ) - Ollama API client with gpt-oss thinking field support
5. **LocalCode::Tools** (23 tests ‚úÖ) - Tool execution with browser integration
6. **LocalCode::UI** (‚úÖ) - Terminal interface + readline + autocompletion
7. **bin/localcode** (‚úÖ) - Main executable with version display (v1.0.0)

### Pending üìã

8. **build.pl** - Single-file distribution builder

## Test Coverage

**310+ tests passing** - Complete TDD implementation with production-ready features

```bash
# Run tests
make test                    # All tests
prove -l t/04-config.t      # Specific module
make test-tui               # TUI automation tests

# Development
make dev                    # Development mode
perl -c lib/LocalCode/*.pm  # Syntax check
```

## Key Features

### üîí Permission System
- **SAFE (0)**: file_read, grep_search, websearch, webopen, webfind - Auto-allowed
- **DANGEROUS (1)**: file_write, shell_exec - Requires confirmation  
- **BLOCKED (2)**: Never allowed

### ü§ñ Ollama Integration
- Auto-detects all available models
- Runtime model switching: `/model llama2`
- **gpt-oss model support** with thinking field display
- Model persistence in ~/.localcode/last_model.txt
- Tab autocompletion for model names
- Model name trimming for robust input
- Fallback to default_model if current unavailable
- Mock mode for testing without Ollama

### üåê Browser Tools (NEW)
- **websearch(query)** - DuckDuckGo web search with instant answers
- **webopen(url_or_id)** - Open webpages or search results by ID
- **webfind(pattern)** - Search text within opened webpages
- Browser state management with page stacks
- SSL certificate bypass for reliable connectivity

### üíæ Session Management
- **Persistent storage** in ~/.localcode directory
- Save/load chat sessions as JSON
- **Unified history** merging chat and command history
- Term::ReadLine integration with cursor key navigation
- History size limits (default: 100 messages)
- Automatic cleanup of old messages

### üõ† Enhanced Tool System
- read(file) - Read files safely
- write(file,content) - Write with permission
- exec(cmd) - Execute commands with permission
- search(pattern,file) - Search in files safely
- websearch(query) - Web search with DuckDuckGo
- webopen(url) - Open and parse webpages
- webfind(pattern) - Search within web content

### üñ• Enhanced TUI Features
**Slash Commands:**
```
/models               # List available models
/model <name>         # Switch to model (with Tab completion)
/current              # Show current model
/tools                # List available tools
/permissions          # Show permission settings
/save <name>          # Save session
/load <name>          # Load session
/sessions             # List saved sessions
/history              # Show unified chat/command history
/version              # Show LocalCode version
/clear                # Clear current session
/help                 # Show help
/exit                 # Exit LocalCode
```

**Interactive Features:**
- **Tab autocompletion** for commands and model names
- **Cursor key navigation** through command history
- **Persistent readline history** across sessions
- **Version display** on startup (v1.0.0)
- **Graceful Ctrl+D exit** with history saving

## Configuration

**config/default.yaml:**
```yaml
ollama:
  host: "localhost"
  port: 11434
  default_model: "codellama"
  current_model: null  # Auto-detect

permissions:
  safe_auto_allow: ["file_read", "grep_search", "websearch", "webopen", "webfind"]
  dangerous_confirm: ["file_write", "shell_exec"]
  
testing:
  auto_approve: false  # For --auto-yes mode
  simulate_only: false # For --simulate mode
```

**Persistent Storage (~/.localcode/):**
```
~/.localcode/
‚îú‚îÄ‚îÄ sessions/              # Saved chat sessions
‚îú‚îÄ‚îÄ last_model.txt         # Last selected model
‚îî‚îÄ‚îÄ command_history        # Readline command history
```

## CLI Testing (LLM Automation)

```bash
# Permission control for autonomous testing
localcode --auto-yes "write hello into hello.txt"
localcode --auto-no "delete config.yaml"  
localcode --simulate "create backup script"
localcode --test-mode "run tests"

# Browser tools testing
localcode --test-mode "websearch Stuttgart"
localcode --test-mode "webopen https://example.com"
localcode --test-mode "webfind hello"

# Model features testing
localcode --set-model "gpt-oss:20b"  # With trimming
localcode --version                  # Version display
localcode --current-model            # Show persisted model

# TUI automation
echo "/models\n/current\n/tools\n/history\n/version\n/exit" | localcode --test-tui-stdin
localcode --test-tui-script test_commands.txt

# Complete test suite
localcode --self-test
```

## System Prompt Injection

**Enhanced with browser tools:**
```
Tools: read(file), write(file,content), exec(cmd), search(pattern,file), websearch(query), webopen(url_or_id), webfind(pattern)
Safe: read,search,websearch,webopen,webfind | Dangerous: write,exec 
Web: websearch("query") ‚Üí [0] result ‚Üí webopen(0) ‚Üí webfind("text") 
Use: read("/path/file") write("/path/file","content") exec("command") search("text","file") websearch("perl modules") webopen(0) webfind("install")
```

## Build System

```bash
make build     # Create dist/localcode (single ~5-10KB file)
make install   # Install to /usr/local/bin/localcode
make clean     # Clean generated files
```

**Development vs Distribution:**
- **Dev**: Modular structure (6 modules √ó ~150 lines)
- **Dist**: Single portable executable with embedded config

## Dependencies

**Minimal CPAN modules:**
- `JSON` - API communication
- `LWP::UserAgent` - HTTP client for Ollama and web tools
- `URI::Escape` - URL encoding for web searches  
- `YAML::Tiny` - Configuration
- `Term::ReadLine::Gnu` - Interactive readline with history
- `Test::More` - Unit testing
- `File::Spec`, `File::Temp`, `File::Path` - File operations

## Current Status

**‚úÖ Completed (Production Ready):**
- Complete project structure
- Comprehensive test suite (9 test files, 310+ tests)
- 7 core modules fully implemented
- Configuration system with ~/.localcode persistence
- Permission management with browser tools
- Ollama client with gpt-oss thinking field support
- Session persistence with unified history
- Enhanced tool execution system
- Browser tools with DuckDuckGo integration
- Interactive UI with readline and autocompletion
- Main executable with version display
- Model persistence and trimming

**üìã Remaining:**
- Build system for single-file distribution
- Integration testing for browser tools

## Performance Targets

- **Code Size**: <1200 lines total ‚úÖ (currently ~1000 lines with browser tools)
- **Startup Time**: <2 seconds ‚úÖ
- **Memory Usage**: <50MB runtime ‚úÖ
- **Test Coverage**: >90% ‚úÖ (currently 100% for implemented modules)
- **Web Search**: <3 seconds average response time ‚úÖ
- **Model Switching**: Instant with persistence ‚úÖ

## Testing Philosophy

**Test-Driven Development:**
1. ‚úÖ Write comprehensive tests first
2. ‚úÖ Implement minimal code to pass tests
3. üîÑ Red ‚Üí Green ‚Üí Refactor cycle
4. üìã Integration testing last

**Autonomous Testing:**
- Every feature has CLI testing interface
- Mock modes for all external dependencies (Ollama, web APIs)
- Automated TUI command testing with browser tools
- SSL certificate bypass for reliable web testing
- LLM can test without human intervention
- gpt-oss model testing with thinking field validation

## Commands for LLM Testing

```bash
# Test individual modules
prove -l t/04-config.t       # Config with persistence
prove -l t/08-permissions.t  # Permissions with browser tools
prove -l t/01-client.t       # Ollama client with gpt-oss support
prove -l t/03-tools.t        # Tools with browser integration
prove -l t/05-session.t      # Sessions with unified history

# Test browser functionality
localcode --test-mode "websearch Stuttgart"
localcode --test-mode "webopen https://example.com"
localcode --test-mode "webfind hello"

# Test model features
localcode --set-model "gpt-oss:20b"  # With trimming
localcode --version                  # Version display

# Test automation
make test-tui                # TUI commands with autocompletion
make test-verbose            # Detailed output

# Syntax validation
perl -c lib/LocalCode/*.pm  # Check all modules
```

This project demonstrates how TDD can create ultra-reliable, minimal code with complete test coverage and autonomous validation capabilities.